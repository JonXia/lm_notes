`LLM.py`：自己设计prompt，

- self-llm或llama factory查找各种llm的特殊token怎么喂入（`build_chat`方法）
- 规定好各种模型的token数量，保证公平
- 喂给llm推理，然后decode llm的output，得到文本

`eval.py`：

评分，使用metrics.py中定义的评分方法，并输出文件

`metrics.py`：

文本规范化:`normalize_zh_aswer``normalize_en_answer`

这些函数将文本小写化，删除标点符号，并处理冠词(例如英语中的“a”、“an”和“the”)。
中文的规范化也去掉了空格。
评分函数:

- classification_score:根据预测类是否与真实类匹配计算一个分数。
- rouge_score和rouge_zh_score:计算ROUGE-L分数(用于评估文本摘要和机器翻译)。
-  f1_score:计算F1分数(用于评估精度和召回率)。
- qa_f1_score和qa_f1_zh_score:专门用于问答任务。

问答(QA):
将中英文分词之后的结果进行比对


`f1_score`

代码首先创建两个 Counter 对象：一个用于prediction，另一个用于ground_truth。
Counter 是一个类似字典的对象，它将元素作为键存储，并将其对应的value作为值存储。
& 运算符用于查找两个 Counter 对象之间的共同元素（键）。
由此产生的common对象包含共享元素及其计数的键值对。

计算共享元素的数量：
下一行将计算prediction和ground_truth之间的共享元素（共同词或标记）的总数。
它将common对象中的value相加。

精确度和召回率：
如果没有共享元素（即 num_same 为 0），函数返回 0。
否则，它将计算精确度和召回率：
精确度precision = （共享元素数量）/（预测元素总数）
召回率recall = （共享元素数量）/ （实际元素总数）
F1 分数：
F1 = (2 * 精确度 * 召回率) / (精确度 + 召回率)
F1 分数平衡了精确度和召回率，提供了评估模型性能的单一指标。