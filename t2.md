# RAG(Retrieval-Augmented Generation) 检索增强技术

> LLM会产生误导性的 “幻觉”，依赖的信息可能过时，处理特定知识时效率不高，缺乏专业领域的深度洞察，同时在推理能力上也有所欠缺



## RAG基本结构

1. 向量化模块，将文本向量化
2. 文档加载和切分模块
3. 数据库存放文档片段和相应的向量表示
4. 检索模块，根据Query检索相关的文档片段
5. 大模型，根据检索出来的文档回答用户问题

![Retrieval-Augmented Generation（RAG-Learning）](pics/t2/Retrieval-Augmented%20Generation%EF%BC%88RAG-Learning%EF%BC%89.png)

### 1. 向量化

实现一个向量化的类：主要是将文本映射为向量，并计算两个向量之间的余弦相似度。

### 2.文档加载和切分

可以加载不同格式的文档（如 pdf、md、txt），并按 Token 长度切分成文档片段，确保片段之间有重叠内容以便于检索。

要注意：

1. chunk与chunk之间最好要有一些重叠的内容，这样才能保证检索的时候能够检索到相关的文档片段，假如问题被切开，回答的内容会不准确；
2. 切分文档的时候最好以句子为单位，也就是按 `\n` 进行粗切分，这样可以基本保证句子内容是完整的。

### 3.向量数据库

用来存放文档片段和向量表示，以及一个检索模块根据Query检索相关文档片段。实现的功能包括数据库持久化、本地保存、加载数据库、获得文档的向量表示和根据问题检索相关文档片段。

对于RAG架构来说，向量数据库的功能最小需要：

- `persist`：数据库持久化，本地保存

- `load_vector`：从本地加载数据库

- `get_vector`：获得文档的向量表示

- `query`：根据问题检索相关的文档片段

  首先先把用户提出的问题向量化，然后去数据库中检索相关的文档片段，最后返回检索到的文档片段。可以看到咱们在向量检索的时候仅使用 `Numpy` 进行加速，代码非常容易理解和修改

  ```py
  def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:
      query_vector = EmbeddingModel.get_embedding(query) # 拿到embedding模型的向量化结果
      result = np.array([self.get_similarity(query_vector, vector)
                          for vector in self.vectors]) # 把相似度最高的放在后面
      return np.array(self.document)[result.argsort()[-k:][::-1]].tolist() # -k从后往前取结果
  ```

### 4.大模型模块

根据检索到的文档回答用户的问题。模型可选从api或者从本地加载。

