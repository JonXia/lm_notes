# RAG(Retrieval-Augmented Generation) 检索增强技术

> LLM会产生误导性的 “幻觉”，依赖的信息可能过时，处理特定知识时效率不高，缺乏专业领域的深度洞察，同时在推理能力上也有所欠缺



## RAG基本结构

1. 向量化模块，将文本向量化
2. 文档加载和切分模块
3. 数据库存放文档片段和相应的向量表示
4. 检索模块，根据Query检索相关的文档片段
5. 大模型，根据检索出来的文档回答用户问题

![Retrieval-Augmented Generation（RAG-Learning）](pics/t2/Retrieval-Augmented%20Generation%EF%BC%88RAG-Learning%EF%BC%89.png)

### 1. 向量化

主要是将文本映射为向量矩阵，并给出计算两个向量余弦相似度的方法，用于后续的检索。

### 2.文档加载和切分

可以加载不同格式的文档（如 pdf、md、txt），并按 Token 长度切分成文档片段，确保片段之间有重叠内容以便于检索。

要注意：

1. 固定分割：以固定长度（例如256/512个tokens）切分，chunk与chunk之间最好要有一些重叠的内容，这样才能保证检索的时候能够检索到相关的文档片段，假如问题被切开，回答的内容会不准确；
2. 句分割：切分文档的时候最好以句子为单位，可以按 `\n` 进行粗切分，这样可以基本保证句子内容是完整的；句分割也可以”句”的粒度进行切分，保留一个句子的完整语义。常见切分符包括：句号、感叹号、问号、换行符等。

### 3.向量数据库

用来存放文档片段和向量表示，以及一个检索模块根据Query检索相关文档片段。实现的功能包括数据库持久化、本地保存、加载数据库、获得文档的向量表示和根据问题检索相关文档片段，常见有FAISS、Chromadb、ES、milvus等。

对于RAG架构来说，向量数据库的功能最小需要：

- `persist`：数据库持久化，本地保存

- `load_vector`：从本地加载数据库

- `get_vector`：获得文档的向量表示

- `query`：根据问题检索相关的文档片段

  首先先把用户提出的问题向量化，然后去数据库中检索相关的文档片段，最后返回检索到的文档片段。下例仅使用 `Numpy` 进行加速

  ```py
  def query(self, query: str, EmbeddingModel: BaseEmbeddings, k: int = 1) -> List[str]:
      query_vector = EmbeddingModel.get_embedding(query) # 拿到embedding模型的向量化结果
      result = np.array([self.get_similarity(query_vector, vector)
                          for vector in self.vectors]) # 把相似度最高的放在后面
      return np.array(self.document)[result.argsort()[-k:][::-1]].tolist() # -k从后往前取结果
  ```

### 4.大模型模块

检索：

- 可以通过计算Query跟所有已存储的Queries余弦相似度，也可用欧氏距离、曼哈顿距离等方法；

- 全文检索：全文检索是一种比较经典的检索方式，在数据存入时，通过关键词构建倒排索引；在检索时，通过关键词进行全文检索，找到对应的记录。

之后根据检索到的content回答用户的问题。

模型可选从api或者从本地加载 。

把检索到的知识和问题，搭配合适的prompt交给模型来处理。

参考：[一文读懂：大模型RAG（检索增强生成） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/675509396)

[github.com/datawhalechina/tiny-universe](https://github.com/datawhalechina/tiny-universe/blob/main/content/TinyRAG/readme.md)

